# Command Generation and Execution MCP (Management Control Program) Specification Document

## 1. Service Introduction
This service is an MCP (Management Control Program) for command generation and execution based on LLM (Large Language Model) and SSH/Telnet protocols. Its core functions include **intelligently generating Linux shell commands based on user needs** and **executing shell commands on local or remote hosts and returning results**. It provides automated support for system administrators to efficiently perform Linux host operation and maintenance tasks (such as information query, resource monitoring, etc.), and supports Chinese-English language configuration switching (controlled by the LanguageEnum enumeration).

## 2. Core Tool Information

| Category | Details |
| -------- | ------- |
| Tool Name | `cmd_generator_tool` |
| Tool Functions | 1. Supports local/remote host scenarios: If `host` (remote host name or IP) is specified, it first obtains remote host system information (including system distribution, uptime, logged-in users, root partition/memory usage, top 5 memory-consuming processes) through SSH protocol; if `host` is not specified, it directly collects local system information<br>2. LLM-based command generation: Passes host system information and user needs (the `goal` parameter, required) to the configured large language model (such as OpenAI-compatible models) to automatically generate Linux shell commands suitable for the scenario<br>3. Command format verification: Extracts YAML format command blocks from LLM return results to ensure valid command strings are output |
| Input Parameters | - `host` (optional): Remote host name or IP address. If not provided, the default operation is on the local machine; the IP, port, username, and password of the remote host need to be pre-configured in the configuration file<br>- `goal` (required): Description of the user's operation and maintenance needs (such as "query root partition usage rate" "view the top 3 processes with the highest memory usage") |

| Category | Details |
| -------- | ------- |
| Tool Name | `cmd_executor_tool` |
| Tool Functions | 1. Command execution scenarios: Supports executing generated shell commands on local or remote hosts<br>2. Remote execution logic: Connects to the remote host through SSH protocol (based on host information in the configuration file), executes commands and captures standard output/error output; automatically closes the SSH connection after execution<br>3. Local execution logic: Executes commands through the `subprocess` module and directly returns execution results<br>4. Error handling: If command execution fails (such as insufficient permissions, command not found), returns specific error information |
| Input Parameters | - `host` (optional): Remote host name or IP address. If not provided, the default operation is on the local machine; it needs to match the remote host information in the configuration file<br>- `command` (required): The Linux shell command string to be executed (it is recommended to be generated by `cmd_generator_tool`) |

## 3. Service Configuration Dependencies
1. **Basic Configuration**: The configuration file needs to be loaded through `CMDGeneratorConfig`, which includes public configuration (`public_config`) and private configuration (`private_config`):
   - Public configuration (`public_config`):
     - `language`: Language enumeration (`LanguageEnum.ZH` for Chinese, others for English), which controls the language of tool names and descriptions
     - `remote_hosts`: List of remote hosts. Each host needs to be configured with `name` (host name), `host` (IP), `port` (SSH port), `username` (login username), `password` (login password)
     - `llm_model`: Name of the large language model (such as `gpt-3.5-turbo`)
     - `llm_remote`: API base address of the LLM service
     - `llm_api_key`: Authentication API Key for the LLM service
     - `max_tokens`: Maximum token limit for LLM requests
     - `temperature`: Randomness of LLM generation results (0~1, lower values mean more deterministic results)
   - Private configuration (`private_config`): The listening port (`port`) of the MCP service, with the default listening address being `0.0.0.0`
2. **Dependent Libraries**: Python libraries such as `paramiko` (SSH connection), `psutil` (system information collection), `langchain-openai` (LLM call), `pyyaml` (configuration parsing), and `mcp.server` (MCP service framework) need to be installed

## 4. To-be-developed Requirements
1. **Command Security Verification**: Add a command whitelist mechanism to filter high-risk operations (such as `rm -rf /`, `shutdown`, etc.) to prevent misoperations or execution of malicious commands; support dynamically adjusting the range of executable commands according to host permissions
2. **Multi-protocol Support**: On the basis of the existing SSH protocol, add Telnet protocol support to adapt to old devices without SSH services
3. **Command Execution Logs**: Add a log module to record the time of command generation/execution, operation host, user needs, command content, and execution results, and support log persistence (such as writing to files or databases) for easy auditing and problem tracing
4. **Batch Operation Capability**: Expand `cmd_executor_tool` to support sending the same command to multiple remote hosts at the same time and aggregating execution results to improve batch operation and maintenance efficiency
5. **LLM Model Optimization**: Support multi-model switching (such as configuring open-source models and commercial models at the same time), add secondary verification logic for model return results, and reduce the error rate of command generation