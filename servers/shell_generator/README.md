# 命令生成与执行MCP（管理控制程序）规范文档
## 一、服务介绍
本服务是一款基于LLM（大语言模型）与SSH/Telnet协议实现命令生成与执行的MCP（管理控制程序），核心功能包括**根据用户需求智能生成Linux shell命令**、**在本地或远程主机执行shell命令并返回结果**，为系统管理员高效完成Linux主机运维操作（如信息查询、资源监控等）提供自动化支撑，支持中英文语言配置切换（基于LanguageEnum枚举控制）。

## 二、核心工具信息
| 类别 | 详情 |
| ---- | ---- |
| 工具名称 | `cmd_generator_tool` |
| 工具功能 | 1. 支持本地/远程主机场景：若指定`host`（远程主机名或IP），则先通过SSH协议获取远程主机系统信息（含系统发行版、运行时间、登录用户、根分区/内存使用情况、Top5内存占用进程）；若不指定`host`，则直接采集本机系统信息<br>2. 基于LLM生成命令：将主机系统信息与用户需求（`goal`参数，必填）传入配置的大语言模型（如OpenAI兼容模型），自动生成符合场景的Linux shell命令<br>3. 命令格式校验：提取LLM返回结果中的YAML格式命令块，确保输出有效命令字符串 |
| 输入参数 | - `host`（可选）：远程主机名称或IP地址，不提供则默认操作本机；需在配置文件中提前配置远程主机的IP、端口、用户名、密码<br>- `goal`（必填）：用户的运维需求描述（如“查询根分区使用率”“查看内存占用最高的3个进程”） |

| 类别 | 详情 |
| ---- | ---- |
| 工具名称 | `cmd_executor_tool` |
| 工具功能 | 1. 命令执行场景：支持在本地或远程主机执行已生成的shell命令<br>2. 远程执行逻辑：通过SSH协议连接远程主机（基于配置文件中的主机信息），执行命令并捕获标准输出/错误输出；执行完成后自动关闭SSH连接<br>3. 本地执行逻辑：通过`subprocess`模块执行命令，直接返回执行结果<br>4. 错误处理：若命令执行出错（如权限不足、命令不存在），则返回具体错误信息 |
| 输入参数 | - `host`（可选）：远程主机名称或IP地址，不提供则默认操作本机；需与配置文件中的远程主机信息匹配<br>- `command`（必填）：需要执行的Linux shell命令字符串（建议由`cmd_generator_tool`生成） |

## 三、服务配置依赖
1. **基础配置**：需通过`CMDGeneratorConfig`加载配置文件，包含公共配置（`public_config`）与私有配置（`private_config`）：
   - 公共配置（`public_config`）：
     - `language`：语言枚举（`LanguageEnum.ZH`为中文，其他为英文），控制工具名称与描述的语言
     - `remote_hosts`：远程主机列表，每个主机需配置`name`（主机名）、`host`（IP）、`port`（SSH端口）、`username`（登录用户名）、`password`（登录密码）
     - `llm_model`：大语言模型名称（如`gpt-3.5-turbo`）
     - `llm_remote`：LLM服务的API基础地址
     - `llm_api_key`：LLM服务的认证API Key
     - `max_tokens`：LLM请求的最大token数限制
     - `temperature`：LLM生成结果的随机性（0~1，值越低结果越确定）
   - 私有配置（`private_config`）：MCP服务的监听端口（`port`），默认监听地址为`0.0.0.0`
2. **依赖库**：需安装`paramiko`（SSH连接）、`psutil`（系统信息采集）、`langchain-openai`（LLM调用）、`pyyaml`（配置解析）、`mcp.server`（MCP服务框架）等Python库

## 四、待开发需求
1. **命令安全性校验**：新增命令白名单机制，过滤高危操作（如`rm -rf /`、`shutdown`等），防止误操作或恶意命令执行；支持根据主机权限动态调整可执行命令范围
2. **多协议支持**：在现有SSH协议基础上，新增Telnet协议支持，适配无SSH服务的老旧设备
3. **命令执行日志**：增加日志模块，记录命令生成/执行的时间、操作主机、用户需求、命令内容、执行结果，支持日志持久化（如写入文件或数据库），便于审计与问题追溯
4. **批量操作能力**：扩展`cmd_executor_tool`，支持同时向多个远程主机发送相同命令并汇总执行结果，提升批量运维效率
5. **LLM模型优化**：支持多模型切换（如同时配置开源模型与商业模型），增加模型返回结果的二次校验逻辑，降低命令生成错误率